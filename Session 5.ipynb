{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef6fd5b-27e3-436b-b3ee-5c9a81b91e80",
   "metadata": {},
   "source": [
    "# Bernoulli  distribution \n",
    "The Bernoulli distribution is used when we have 2 possible outcomes such as flipping a coin, or in cases when we have success or failure. If a random variable X follows a Bernoulli distribution with probability P where is the probability of success or heads:\n",
    "$$ X \\sim B(p)$$ $$ P(X=1)= p$$ $$P(X=0)= 1-p $$ \n",
    "We can write this as a function for all different outcomes:\n",
    "$$ f(X= x|P)= f(x|P)$$\n",
    "> In this notation, the capital letter refers to a random variable, the lowercase letter refers to the possible value is taken and p is the specified probability.\n",
    "In the case of Bernoulli:\n",
    "$$ f(x|p)= P^x .(1-p)^{1-x}.\\mathbb{I_{x\\in [0,1]}}(x), \\mathbb{I_{x\\in [0,1]}}(x) = \\begin{cases} P & \\text{if } X=1 \\\\ 1-P & \\text{if } X=0 \\end{cases}$$\n",
    "> One way to write this function is to use indicator function. In this function, the x is either 0 or 1. This indicator function is a step function, also known as a **heavy side functoin**.\n",
    "\n",
    "This function takes value 1 when its argument is true and it takes 0 when its argument is false. The indicator function takes in the order of operation, so we always evaluated it first. This is a way we can avoid doing things such as taking the $\\log(x)$ or the square root of negative numbers.\n",
    "\n",
    "For the probability mass function (**PMF**) for the Bernoulli distribution:\n",
    "$$Pmf (X=x)= P^x .(1-p)^{1-x}{ \\mathbb{I_{x\\in [0,1]}}(x)} \\rightarrow Pmf(X=x)= \\mathbb{I}(X=1) = \\begin{cases} P^2 & \\text{if } x=1 \\\\ (1-P)^2 & \\text{if } x=0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1da39d-a58e-469f-945a-ac3e0dbc9d6b",
   "metadata": {},
   "source": [
    "## Expected Value\n",
    "This is the theoretical average or the theoretical mean:\n",
    "$$ E(x)= \\sum_x{ x}{ P(X=x)}$$\n",
    "In the Bernoulli distribution, the expected value can be calculated as below:\n",
    "$$ E(x)=  \\begin{cases} P & \\text{if } x=1  \\\\ 0 & \\text{if } x=0  \\end{cases}$$\n",
    "## Variance\n",
    "Variance is the square root of the standard deviation. For the Bernoulli distribution:\n",
    "$$ Var(x)= \\sum_x (x-\\mu)^2{ P(X=x)} \\rightarrow Var(x)= p. (1-p)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e152af-26dd-4bb4-ac8c-2bf440f1d72e",
   "metadata": {},
   "source": [
    "# Binomial Distribution \n",
    "The generalization of the Bernoulli when we have N repeated trials is Binomial.In other words, Binomial is the sum of N independent Bernoulli.\n",
    "$$ X\\sim Bin(n,p)$$\n",
    "$$ P(X=x|p)= f(X=x|p)= P(X = k) = \\binom{n}{x} p^x (1-p)^{n-x}$$\n",
    "The $\\binom{n}{x}$ is the common term and is computed as:\n",
    "$$ \\binom {n}{x}= \\frac {n!}{x!(n-x)!}\\text{ for } x\\in [0,1,...,n]$$\n",
    "The $ \\binom{n}{x}$ Commonly read as **n choose x**. This is combination term counts all possible Bernoulli sequences that results in x success out of n trials. For example, in the $\\binom {4}{3}$, we have four sequences [1,1,1,0], [1,0,1,1], [1,1,0,1], [0,1,1,1].\n",
    "\n",
    "The **Expected value** and the **Vairance** for the binomial distribution are as below respectively:\n",
    "$$ E(x)= np$$\n",
    "$$ Var(x)= np(1-p)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd6a6a-9150-4ad2-8d7d-c0a77b27e4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
